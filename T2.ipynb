{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador de tênis (Nike vs Adidas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carregamento de dados dos datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def datasetLoader(data_dir, batch_size):\n",
    "    transform = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])\n",
    "    train_dir = os.path.join(data_dir, \"NewTrain\")\n",
    "    train_data = ImageFolder(root=train_dir, transform=transform)\n",
    "\n",
    "    # Get the indices of train and test samples\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(train_data)), test_size=0.33, random_state=42\n",
    "    )\n",
    "\n",
    "    # Create data subsets\n",
    "    train_subset = Subset(train_data, train_indices)\n",
    "    test_subset = Subset(train_data, test_indices)\n",
    "\n",
    "    # Create data loaders\n",
    "    training_loader = DataLoader(\n",
    "        train_subset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_subset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return training_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"Dataset\"\n",
    "train_loader, test_loader = datasetLoader(data_dir, batch_size=12)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de amostras de treinamento: 377\n",
      "Número de amostras de teste: 187\n"
     ]
    }
   ],
   "source": [
    "print(f\"Número de amostras de treinamento: {len(train_loader.dataset)}\")\n",
    "print(f\"Número de amostras de teste: {len(test_loader.dataset)}\")\n",
    "#print(\"Classes no conjunto de treinamento:\")\n",
    "#print(train_loader.dataset.samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resnet = torchvision.models.resnet50(weights = True)\n",
    "resnet.fc = nn.Linear(2048, 2)\n",
    "resnet = resnet.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métodos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matriz de confusão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(model, loader):\n",
    "    model.eval()\n",
    "    confusion_matrix = np.zeros((2,2))\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs, 1)\n",
    "            for i in range(labels.size(0)):\n",
    "                confusion_matrix[labels[i].item()][predicted[i].item()] += 1\n",
    "    ax = sns.heatmap(confusion_matrix, annot=True, cmap='Blues', fmt='g', xticklabels=['Adidas', 'Nike'], yticklabels=['Adidas', 'Nike'])\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Label')\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acurácia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(model, loader):\n",
    "    model.eval()\n",
    "    corrected = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _,predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            corrected += (predicted == labels).sum().item()\n",
    "    return corrected * 100 // total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calculate_f1_score(model, loader):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "    return f1_score(true_labels, predicted_labels, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC AUC métrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "def calculate_roc_auc_score(model, loader, n_classes):\n",
    "    model.eval()\n",
    "    true_labels = []\n",
    "    predicted_probs = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            probs = torch.nn.functional.softmax(outputs, dim=1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_probs.extend(probs.cpu().numpy())\n",
    "    true_labels = label_binarize(true_labels, classes=[i for i in range(n_classes)])\n",
    "    return roc_auc_score(true_labels, predicted_probs, multi_class='ovr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs,labels)\n",
    "            val_loss +=loss\n",
    "    return val_loss/len(loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularização"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "l1_lambda e l2 lambda são os pesos/intensidade que a regularização aplicará no modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l1_regularization(model, l1_lambda, device):\n",
    "    l1_reg = torch.tensor(0., device=device)\n",
    "    for param in model.parameters():\n",
    "        # Calculando a norma L1 dos parâmetros e somando-as\n",
    "        l1_reg += torch.norm(param, 1)\n",
    "    # Multiplicando pela lambda para obter o termo de regularização L1\n",
    "    return l1_lambda * l1_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_regularization(model, l2_lambda, device):\n",
    "    l2_reg = torch.tensor(0., device=device)\n",
    "    for param in model.parameters():\n",
    "        # Calculando a norma L2 dos parâmetros e somando suas raízes quadradas\n",
    "        l2_reg += torch.norm(param, 2) ** 2\n",
    "    # Multiplicando pela lambda e raiz quadrada para obter o termo de regularização L2\n",
    "    return l2_lambda * torch.sqrt(l2_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, trainloader, testloader, optimizer, criterion, epochs, l1_lambda, l2_lambda):\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0\n",
    "        for data in tqdm(trainloader):\n",
    "            images, labels = data\n",
    "            optimizer.zero_grad()\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            l1_reg = l1_regularization(model, l1_lambda, device)\n",
    "            loss += l1_reg\n",
    "            l2_reg = l2_regularization(model, l2_lambda, device)\n",
    "            loss += l2_reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        val_loss = validation(model, testloader, criterion)\n",
    "        print(f'Epoch: {epoch+1} | Loss: {running_loss/len(trainloader)} | Val Loss: {val_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Registro de evoluções nos modelos\n",
    "- 1º modelo era treinado com apenas 3 épocas e com learning rate de 0,001. **Acurácia de 70%**.\n",
    "- 2º modelo subimos o número de épocas para 30 (número baseado nos modelos analisados da referência 1) e ajustamos o learning rate para 0,0001. **Acurácia de 88%**.\n",
    "- 3º modelo inserimos regularização L1 e L2 com peso da regularização de 0,01. **Acurácia caiu para 76%**\n",
    "- 4º modelo ajustamos os parâmetros de pesos da regularização L1 e L2 para de 0,00001. **Acurácia de 90%**\n",
    "\n",
    "Referências:  \n",
    "1 - https://www.kaggle.com/datasets/ifeanyinneji/nike-adidas-shoes-for-image-classification-dataset/code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(resnet.parameters(), lr=0.0001)\n",
    "l1_lambda = 0.0001\n",
    "l2_lambda = 0.0001\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 10/32 [00:04<00:08,  2.68it/s]c:\\Users\\guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\PIL\\Image.py:975: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "100%|██████████| 32/32 [00:11<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 27.448494493961334 | Val Loss: 0.35553136467933655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 | Loss: 26.194783926010132 | Val Loss: 0.16501106321811676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 | Loss: 25.14391154050827 | Val Loss: 0.1407821625471115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 | Loss: 24.05941081047058 | Val Loss: 0.39362236857414246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 | Loss: 23.013822734355927 | Val Loss: 0.1599707007408142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 | Loss: 21.924455404281616 | Val Loss: 0.1843753159046173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 | Loss: 20.90865331888199 | Val Loss: 0.16543565690517426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:10<00:00,  2.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8 | Loss: 19.890487670898438 | Val Loss: 0.18289776146411896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9 | Loss: 18.94516009092331 | Val Loss: 0.18909813463687897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10 | Loss: 18.015734910964966 | Val Loss: 0.15022948384284973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11 | Loss: 17.254152953624725 | Val Loss: 0.2124256044626236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12 | Loss: 16.636413097381592 | Val Loss: 0.48872002959251404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13 | Loss: 16.367608308792114 | Val Loss: 0.2619026303291321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 | Loss: 15.73658737540245 | Val Loss: 0.3987376093864441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15 | Loss: 15.000255316495895 | Val Loss: 0.10305337607860565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16 | Loss: 14.509858906269073 | Val Loss: 0.1970970332622528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17 | Loss: 13.94856134057045 | Val Loss: 0.131090447306633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18 | Loss: 13.367806226015091 | Val Loss: 0.11318102478981018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19 | Loss: 12.841779738664627 | Val Loss: 0.14967241883277893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20 | Loss: 12.381237775087357 | Val Loss: 0.09927904605865479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 21 | Loss: 11.996837168931961 | Val Loss: 0.12823975086212158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 22 | Loss: 11.756266832351685 | Val Loss: 0.13231509923934937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 23 | Loss: 11.634063810110092 | Val Loss: 0.197630375623703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 24 | Loss: 11.109082877635956 | Val Loss: 0.13458223640918732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25 | Loss: 10.611369103193283 | Val Loss: 0.09951341897249222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 26 | Loss: 10.216054499149323 | Val Loss: 0.09679686278104782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 27 | Loss: 10.78640529513359 | Val Loss: 0.3310048282146454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 28 | Loss: 10.305842399597168 | Val Loss: 0.20773418247699738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 29 | Loss: 9.741988360881805 | Val Loss: 0.18369880318641663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:11<00:00,  2.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30 | Loss: 9.299201220273972 | Val Loss: 0.16012293100357056\n"
     ]
    }
   ],
   "source": [
    "train(resnet, train_loader, test_loader, optimizer, criterion, epochs=epochs, l1_lambda=l1_lambda, l2_lambda=l2_lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Métricas de avaliação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A rede atinge: 93% de acurácia\n",
      "A rede atinge: 93.04% de recall\n",
      "A rede atinge: 98.5% de ROC AUC Score\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'A rede atinge: {accuracy(resnet, test_loader)}% de acurácia')\n",
    "print(f'A rede atinge: {round(calculate_f1_score(resnet, test_loader)*100,2)}% de recall')\n",
    "print(f'A rede atinge: {round(calculate_roc_auc_score(resnet, test_loader, 50)*100,2)}% de ROC AUC Score')\n",
    "conf_mat = confusion_matrix(resnet, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usabilidade de webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "img_counter = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    cv2.imshow('Webcam', frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    elif cv2.waitKey(1) & 0xFF == ord('e'):\n",
    "            img_name = \"WebcamImages/print_{}.png\".format(img_counter)\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            print(\"{} written!\".format(img_name))\n",
    "            img_counter += 1\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, image):\n",
    "    prediction = model(torch.unsqueeze(image, 0).to(device))\n",
    "    result = torch.argmax(prediction)\n",
    "    return 'Adidas' if result == 0 else 'Nike'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predição das classes da webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: print_1.png | Resnet Prediction: Nike\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "for filename in os.listdir('WebcamImages'):\n",
    "    if filename.endswith(\".png\"):\n",
    "        x = Image.open('WebcamImages/' + filename).convert('RGB')\n",
    "        x = transforms.Compose([transforms.Resize((256, 256)), transforms.ToTensor()])(x)\n",
    "        print(f'Image: {filename} | Resnet Prediction: {predict(resnet, x)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
